以下、現時点までの前提（**リファレンス=2mix/Stem両対応**、**新規ボーカル=同じ曲を別の人が歌唱**、**Windows/mac対応**、**将来VST3**）を踏まえて、企画〜実装まで一気に通る形で「資料」として整理しました。

---

# 1. 企画概要

## 1.1 目的

* Mix工程で最も時間を食う **ボーカルエディット（ピッチ/タイミング）**を、リファレンス（過去Mix or Stem）に合わせて自動化し、手直しを最小化する。

## 1.2 想定アウトプット

* スタンドアロンアプリ（Win/mac）

  * 入力：リファレンス（2mix or Vocal Stem）＋新規ドライボーカル
  * 出力：補正済みボーカルWAV＋編集レシピ（JSON）
* 将来：VST3（オフライン解析＋プラグイン適用）

---

# 2. 前提条件（重要）

## 2.1 入力条件

* リファレンス：

  * **2mix**（ボーカル分離が必要）
  * **Stem（vocal stem）**（分離不要、精度が上がる）
* 新規ボーカル：

  * **同じ曲**だが **別の歌い手**が歌う想定
  * 声域・歌い回し（フェイク/語尾/子音/ブレス）はズレる前提

## 2.2 成功の定義（実用ライン）

* 100%自動で完璧を狙わず、
  **“8割自動→残り2割だけ短時間で手直し”**を狙う。

---

# 3. 全体処理フロー（パイプライン）

1. **入力読込**（WAV/AIFF中心、MVPはこれで十分）
2. **（2mixの場合）ボーカル分離** → リファレンスvocal生成
3. **解析**（リファレンスvocal / 新規vocal 両方）

   * F0（ピッチカーブ）
   * オンセット（発声立ち上がり）
   * ノート/音節区間（粗くてOK）
   * 無声音（子音）・ブレス区間（簡易推定）
4. **アライメント（対応付け）**

   * 第1軸：拍/テンポ構造（曲構造）
   * 第2軸：歌詞/音素（あれば強い）
   * 第3軸：F0/スペクトル特徴（保険）
5. **編集レシピ生成（JSON）**

   * タイムワープマップ（時間の対応）
   * 目標ピッチカーブ（または目標ノート中心）
   * 区間ごとの信頼度（confidence）と自動ブレーキ情報
6. **適用レンダリング**

   * タイムストレッチ（区間ワープ）
   * ピッチシフト（目標へ寄せる）
   * 子音/無声音保護（自然さ維持）
7. **出力**

   * edited_vocal.wav
   * recipe.json
   * ログ（任意）

---

# 4. 機能要件

## 4.1 MVPで必須

* 2mix/Stemどちらも入力できる
* 自動エディット（ピッチ/タイミング）
* プリセット（軽め/標準/強め）
* 書き出し（WAV）＋レシピJSON保存
* 破綻しそうな区間は **自動で弱める**（やりすぎ防止）

## 4.2 v1で入れたい（効率が跳ねる）

* 波形＋区間（最低限の可視化）
* 破綻リスク区間のハイライト
* 区間ごとの「補正強度」だけ触れる簡易編集
* バッチ処理（複数テイク）

## 4.3 将来（VST3向け）

* 事前解析（スタンドアロン/CLI）で recipe.json を生成
* プラグイン側で適用＋軽微な調整・プレビュー

---

# 5. 非機能要件

* 対応OS：Windows / mac（Apple Silicon優先、可能ならUniversal）
* 品質：子音が崩れない／ワープ感が過剰にならない
* 時間：3〜5分曲を数分程度で処理（GPUで短縮）
* 再現性：同一入力→同一出力
* 例外耐性：無音・ノイズ・分離失敗でも落ちない

---

# 6. アーキテクチャ（mac/将来VST3を見据えた形）

## 6.1 推奨：二層構成（解析と適用を分離）

* **解析エンジン（CLI/ライブラリ）**

  * 分離 → 解析 → アライメント → recipe.json生成
* **適用エンジン（レンダラ）**

  * 新規vocal + recipe.json → 高音質変換 → WAV出力
* **GUIアプリ**

  * 入力/設定/実行/可視化/簡易編集

この分離により：

* mac配布が安定
* VST3化で「重い解析」をプラグイン外へ逃がせる

---

# 7. 採用予定ライブラリ／学習モデル（現時点）

## 7.1 ボーカル分離（2mix時）

* **Demucs v4（HTDemucs系）**

  * 2mix→vocal抽出のベースラインとして採用候補

## 7.2 ピッチ（F0）推定

* **RMVPE**（第一候補）

  * 分離残渣があってもF0を取りやすい方向の選択
* 併用候補：CREPE系（バックアップ）

## 7.3 拍/オンセット・曲構造の推定

* **madmom**（ビート/オンセット系の土台候補）
* 代替ルート：librosa等（環境相性の保険）

## 7.4 歌詞/音素アライメント（精度を上げる“第2軸”）

* 歌詞が用意できる運用なら：**Montreal Forced Aligner**
* 歌詞がない/補助なら：**WhisperX**（推定→アライメント補助）

> 「別の歌い手」前提では、声質ではなく **歌詞/音節境界**が効くので、可能なら歌詞テキスト運用を推奨。

## 7.5 変換適用（音質の要）

* **Rubber Band Library**

  * タイムストレッチ＆ピッチシフトの中核

## 7.6 推論ランタイム（製品化・配布安定）

* MVP：PyTorch中心で検証
* 製品化：ONNX Runtimeへ寄せる
* mac最適化：Apple環境で必要ならCore MLも検討

## 7.7 GUI／将来VST3

* GUI/クロスプラットフォーム：JUCE
* VST3：Steinberg SDK（JUCE経由）

---

# 8. 編集レシピ（JSON）設計案

## 8.1 最小スキーマ（MVP）

* `sample_rate`
* `global_key_shift_semitones`（別歌唱でキー差が出るため）
* `segments[]`

  * `t0, t1`
  * `time_warp_points[]`（入力→出力の対応点）
  * `pitch_target_curve[]`（時間とcents/Hz）
  * `confidence`（一致度）
  * `protect_unvoiced`（子音保護フラグ）
* `warnings[]`（破綻リスク区間）

> これを中心資産にすると、GUI改善やVST3移行でも互換を保てます。

---

# 9. mac対応の追加設計（配布で詰まらないために）

* 依存バイナリ同梱を最小化（MVPはWAV/AIFF限定でOK）
* Universal Binaryを目標（arm64/x86_64）
* 署名・公証（Notarization）はv1以降で必須化する想定

---

# 10. リスクと対策（別歌唱前提で必ず出る問題）

## 10.1 分離の誤差（2mix時）

* 対策：Stem入力を最優先、分離は差し替え可能設計

## 10.2 別歌唱によるアライメント破綻（フェイク差）

* 対策：

  * 拍→歌詞→F0 の多段アライメント
  * confidenceで自動弱め（“やりすぎない”）

## 10.3 子音の崩れ（自然さの最大の敵）

* 対策：無声音検出→保護、母音中心に補正適用

---

# 11. 実装計画（マイルストーン）

## Phase 0：検証（最短）

* 1曲でパイプライン成立

  * 分離（任意）→ F0 → アライメント → 適用 → WAV出力
* 評価：編集時間がどれだけ減るか、破綻が局所化するか

## Phase 1：MVP（スタンドアロン）

* GUI（入出力/プリセット/実行/ログ）
* recipe.json保存
* WAV書き出し
* 破綻検知＋自動弱め

## Phase 2：実用化

* 可視化（区間/ノート）
* 区間ごとの強度調整
* バッチ処理
* エラー耐性・速度改善

## Phase 3：配布品質（Win/mac）

* ONNX Runtime化（推論安定）
* mac署名/公証の整備

## Phase 4：VST3

* 事前解析（外部）＋プラグイン適用の分業
* DAWプロジェクトとレシピ連携

---

# 12. 成果物（ドキュメント／実装）

* 要件定義（本資料をベースに確定版）
* recipe.json スキーマ（固定）
* 解析CLI（分離/解析/アライメント/レシピ生成）
* 適用レンダラ（高音質変換）
* GUI（Win/mac）
* 評価レポート（曲ごとの成功率、手直し箇所、時間短縮）

---

必要なら次のステップとして、この資料を「実装に直行できる粒度」に落とします：

* **モジュール分割（クラス/フォルダ構成）**
* **recipe.json の正式JSON Schema**
* **MVPのUIワイヤー**
* **テスト用データセット設計（あなたの過去曲での評価手順）**

どれから作るのが一番助かりますか？（おすすめは *recipe.jsonスキーマ確定* → *Phase0検証* の順です）

予定している構成は「**分離**」「**解析（F0/ノート/オンセット）**」「**アライメント**」「**適用（ピッチ/タイム）**」を分けて、各段で“強い定番”を使う方針です。MVP→製品化→VST3の順に、採用候補を具体名でまとめます。

---

## 1) ボーカル分離（リファレンス2mix → vocal抽出）

### 第一候補：**Demucs v4（Hybrid Transformer / htdemucs系）**

* 理由：品質と実績が強く、vocal抽出のベースとして堅い。公式リポジトリでもv4のHybrid Transformerが説明されています。([GitHub][1])
* 運用：モデルは差し替え可能にして、曲ジャンルや音源条件で最適化

### 代替/拡張（将来）

* MDX系（UVR周辺でよく使われる系列）
  ※モデルや実装が増えやすいので「モジュール差し替え」にしておくのが長寿命

---

## 2) ピッチ（F0）推定 / ノート化（編集の核）

### 第一候補：**RMVPE（Robust Multi-View Pitch Estimation）**

* 理由：ポリフォニック（伴奏残り・分離残渣あり）でもボーカルF0を取りやすいことを狙った研究モデルで、歌ものに相性が良い。([arXiv][2])
* 使い方：

  * リファレンス抽出vocal → F0カーブ
  * 新規ドライvocal → F0カーブ
  * これを後段のアライメントと補正目標に使う

### 第二候補：**CREPE系（torchcrepe）**

* 理由：実装が手堅く、推論も扱いやすい。([GitHub][3])
* 位置づけ：RMVPEが合わないケースのバックアップ／比較用

### 低依存バックアップ（軽量ルート）

* `torchaudio.functional.detect_pitch_frequency`（相関ベース）
  → 精度は落ちるが依存が軽く非常用に便利。([PyTorch Docs][4])

### ノート化（F0→ノート境界）

* まずはルールベース（F0安定区間＋エネルギー変化＋オンセット）でMVPを作る
* v1以降で「子音/ブレス」を推定してノート境界の破綻を減らす

---

## 3) アライメント（リファレンス ↔ 新規の対応付け）

### 第一候補：**DTW（Dynamic Time Warping）**

* 入力特徴：F0（cents）＋オンセット列＋スペクトル特徴（MFCC等）
* 理由：「歌い回しが近い別テイク」を想定するなら、まずDTWが最短で強い

### v1以降（精度上げ）

* 「破綻検知」：一致度が低い区間は自動で補正強度を落とす（やりすぎ防止）

---

## 4) 変換適用（タイムストレッチ / ピッチシフト：音質の決め手）

### 第一候補：**Rubber Band Library**

* 理由：高品質のタイムストレッチ＆ピッチシフトの定番で、公式もその用途を明記。([breakfastquay.com][5])
* 方針：

  * タイミングは“区間ワープ”として適用
  * ピッチは“目標カーブ”へ寄せる（ビブラート保持・過補正抑制）
  * 無声音（子音）保護は最優先で設計

---

## 5) 実装言語・配布（Win/mac対応を前提）

### MVP（早く形にする）

* 解析・モデル推論：Python（PyTorch系）でまず成立させる
* アプリ：GUIは最初からクロスプラットフォームで作るなら JUCE が最有力

  * 解析は外部CLI呼び出し（JSON + WAV）でもOK

### 製品化（mac配布がラクになる）

* 推論ランタイム：**ONNX Runtime** へ寄せる（Win/macで同じ配布がしやすい）
* mac最適化を突き詰めるなら：Core ML（後で追加）

### VST3（将来）

* Steinberg VST3 SDK＋JUCEでプラグイン化
* ただし重い「分離/解析」は事前（スタンドアロン）に寄せ、プラグインは「適用＋微調整」にする予定

---

## 6) まとめ：現時点の“採用予定セット”

* 分離：**Demucs v4（htdemucs）** ([GitHub][1])
* F0：**RMVPE（第一）／torchcrepe（第二）** ([arXiv][2])
* アライメント：**DTW（F0＋オンセット＋スペクトル）**
* 変換：**Rubber Band Library** ([breakfastquay.com][5])
* クロスプラットフォーム：**JUCE（GUI/将来VST3）＋ONNX Runtime（推論）**

---

もし「あなたの用途」に合わせて選定を確定させるなら、次の2点で“最適解”が変わります（答えがなくても作れますが、精度と手戻りが大きく変わります）：

1. リファレンスは **2mixのみ**？それとも **vocal stem（過去の書き出し）**がありますか？
2. 新規ボーカルは **同じ曲の別テイク（歌詞/構成が一致）**が前提ですか？

[1]: https://github.com/facebookresearch/demucs?utm_source=chatgpt.com "Demucs: Hybrid Spectrogram and Waveform Source Separation"
[2]: https://arxiv.org/abs/2306.15412?utm_source=chatgpt.com "A Robust Model for Vocal Pitch Estimation in Polyphonic ..."
[3]: https://github.com/maxrmorrison/torchcrepe?utm_source=chatgpt.com "maxrmorrison/torchcrepe: Pytorch implementation of the ..."
[4]: https://docs.pytorch.org/audio/main/generated/torchaudio.functional.detect_pitch_frequency.html?utm_source=chatgpt.com "torchaudio.functional.detect_pitch_frequency"
[5]: https://breakfastquay.com/rubberband/?utm_source=chatgpt.com "Rubber Band Audio Time Stretcher Library"
